{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Dimensional Arrays and a Little More Broadcasting\n",
    "\n",
    "\n",
    "A high dimensional array has multiple indices: $A[i_1,i_2,...,i_k]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "A = np.random.randn(2,3,4,3)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-1.0856306   0.99734545  0.2829785 ]\n",
      "   [-1.50629471 -0.57860025  1.65143654]\n",
      "   [-2.42667924 -0.42891263  1.26593626]\n",
      "   [-0.8667404  -0.67888615 -0.09470897]]\n",
      "\n",
      "  [[ 1.49138963 -0.638902   -0.44398196]\n",
      "   [-0.43435128  2.20593008  2.18678609]\n",
      "   [ 1.0040539   0.3861864   0.73736858]\n",
      "   [ 1.49073203 -0.93583387  1.17582904]]\n",
      "\n",
      "  [[-1.25388067 -0.6377515   0.9071052 ]\n",
      "   [-1.4286807  -0.14006872 -0.8617549 ]\n",
      "   [-0.25561937 -2.79858911 -1.7715331 ]\n",
      "   [-0.69987723  0.92746243 -0.17363568]]]\n",
      "\n",
      "\n",
      " [[[ 0.00284592  0.68822271 -0.87953634]\n",
      "   [ 0.28362732 -0.80536652 -1.72766949]\n",
      "   [-0.39089979  0.57380586  0.33858905]\n",
      "   [-0.01183049  2.39236527  0.41291216]]\n",
      "\n",
      "  [[ 0.97873601  2.23814334 -1.29408532]\n",
      "   [-1.03878821  1.74371223 -0.79806274]\n",
      "   [ 0.02968323  1.06931597  0.89070639]\n",
      "   [ 1.75488618  1.49564414  1.06939267]]\n",
      "\n",
      "  [[-0.77270871  0.79486267  0.31427199]\n",
      "   [-1.32626546  1.41729905  0.80723653]\n",
      "   [ 0.04549008 -0.23309206 -1.19830114]\n",
      "   [ 0.19952407  0.46843912 -0.83115498]]]]\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9274624317585825\n"
     ]
    }
   ],
   "source": [
    "print(A[0,2,3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have operations such as suming one of the dimensions. Suppose we sum the third dimension:\n",
    "\n",
    "\\begin{equation}\n",
    "    B[i_1,i_2,i_3,i_4] = \\sum_{i_3} A[i_1,i_2,i_3,i_4]\n",
    "\\end{equation}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "    B[i_1,i_2,i_4] = \\sum_{i_3} A[i_1,i_2,i_3,i_4]\n",
    "\\end{equation}\n",
    "\n",
    "Depending on if we \"keep the dimension\" or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 1, 3)\n",
      "(2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(A.sum(axis=2,keepdims=True).shape)\n",
    "print(A.sum(axis=2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can do element wise operations on ndarrays or on pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7302808795242763 0.7302808795242763\n",
      "0.2844197299577539 0.2844197299577539\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "A = np.random.rand(2,3,4,2)\n",
    "Z = np.sqrt(A)\n",
    "print(Z[1,2,2,1],np.sqrt(A[1,2,2,1]))\n",
    "\n",
    "Z = A**2\n",
    "print(Z[1,2,2,1],(A[1,2,2,1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9475212850328238 -0.9475212850328238\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(2,3,4,2)\n",
    "B = np.random.randn(2,3,4,2)\n",
    "\n",
    "Z = A*B\n",
    "print(Z[1,2,2,1],(A[1,2,2,1]*B[1,2,2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why this way?\n",
    "A.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  8, 12,  1,  5,  9, 13,  2,  6, 10, 14,  3,  7, 11, 15])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reshape(-1,4).T.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Broadcasting examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0856306   0.99734545]\n"
     ]
    }
   ],
   "source": [
    "A1 = np.random.randn(d)\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "A2 = np.ones((d,d))\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0856306   0.99734545]\n",
      " [-1.0856306   0.99734545]]\n"
     ]
    }
   ],
   "source": [
    "print(A2 * A1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]]\n",
      "\n",
      " [[-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]]\n",
      "\n",
      " [[-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]]\n",
      "\n",
      " [[-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]]]\n"
     ]
    }
   ],
   "source": [
    "B = np.ones((d+2,d+1,d))\n",
    "print(B*A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0856306   0.99734545]]\n",
      "(1, 2)\n",
      " \n",
      "[[[-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]]\n",
      "\n",
      " [[-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]]\n",
      "\n",
      " [[-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]]\n",
      "\n",
      " [[-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]\n",
      "  [-1.0856306   0.99734545]]]\n"
     ]
    }
   ],
   "source": [
    "A4 = A1.reshape(1,-1)\n",
    "print(A4)\n",
    "print(A4.shape)\n",
    "print(\" \")\n",
    "print(B*A4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0856306 ]\n",
      " [ 0.99734545]]\n",
      " \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,3,2) (2,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7af79f54721c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# OOOOOPPPPPPSSSSS.....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,3,2) (2,1) "
     ]
    }
   ],
   "source": [
    "A5 = A1.reshape(-1,1)\n",
    "print(A5)\n",
    "print(\" \")\n",
    "print(B*A5)\n",
    "# OOOOOPPPPPPSSSSS....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 2)\n",
      "(4, 3, 2)\n",
      "[[[ 0.2829785  -1.50629471]\n",
      "  [ 0.2829785  -1.50629471]\n",
      "  [ 0.2829785  -1.50629471]]\n",
      "\n",
      " [[-0.57860025  1.65143654]\n",
      "  [-0.57860025  1.65143654]\n",
      "  [-0.57860025  1.65143654]]\n",
      "\n",
      " [[-2.42667924 -0.42891263]\n",
      "  [-2.42667924 -0.42891263]\n",
      "  [-2.42667924 -0.42891263]]\n",
      "\n",
      " [[ 1.26593626 -0.8667404 ]\n",
      "  [ 1.26593626 -0.8667404 ]\n",
      "  [ 1.26593626 -0.8667404 ]]]\n"
     ]
    }
   ],
   "source": [
    "A6 = np.random.randn(d+2,1,d)\n",
    "print(A6.shape)\n",
    "print(B.shape)\n",
    "print(A6*B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most operations, when broadcasting,\n",
    "- dimension of length 1 behave as if they were filled with copies.\n",
    "- an array with fewer dimensions behave as if we added \"1\" dimensions to the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "A=np.random.randn(2,5,3,3)\n",
    "# what does this mean for a n-d array????\n",
    "B = la.inv(A)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -4.68393428e-16,  5.89805982e-17],\n",
       "       [-4.61865832e-15,  1.00000000e+00, -5.41233725e-16],\n",
       "       [ 1.42108547e-14, -3.55271368e-15,  1.00000000e+00]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treated as an array of matrices\n",
    "B[0,2]@A[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1046e-01,  7.7046e-01, -1.7830e-01],\n",
       "          [-3.0832e-01, -1.3149e-01,  6.5463e-01],\n",
       "          [-1.1778e+00,  7.6865e-01, -1.5258e-01]],\n",
       "\n",
       "         [[-4.7080e+00, -1.1009e+01,  8.5325e+00],\n",
       "          [ 3.6516e+00,  7.0143e+00, -6.3237e+00],\n",
       "          [ 2.5439e+00,  5.3896e+00, -5.7215e+00]],\n",
       "\n",
       "         [[-7.5867e-01,  4.4421e-01, -1.2191e-01],\n",
       "          [ 1.8003e+00,  7.0193e-01, -4.4400e-01],\n",
       "          [-9.5672e-02,  4.2028e-01,  3.7353e-01]],\n",
       "\n",
       "         [[-1.6259e+01, -4.2046e+01, -6.3759e+01],\n",
       "          [ 8.9497e+00,  1.9664e+01,  2.9970e+01],\n",
       "          [ 2.5149e+00,  6.5585e+00,  9.5306e+00]],\n",
       "\n",
       "         [[ 1.4515e-01, -1.8653e+00,  3.3811e+00],\n",
       "          [-1.5749e+00, -3.9425e+00,  7.8881e+00],\n",
       "          [ 4.9875e-01,  1.8706e+00, -1.0130e+01]]],\n",
       "\n",
       "\n",
       "        [[[-9.1470e-01, -5.3970e+00,  4.9804e+00],\n",
       "          [ 1.5210e-01, -2.7255e+00,  2.1599e+00],\n",
       "          [ 7.3451e-01,  3.0182e+00, -2.2579e+00]],\n",
       "\n",
       "         [[-7.8159e+00, -2.1662e+01,  5.4915e+00],\n",
       "          [ 1.3738e+01,  4.1233e+01, -1.1345e+01],\n",
       "          [ 1.9034e+01,  5.5516e+01, -1.5565e+01]],\n",
       "\n",
       "         [[-3.7710e-01,  5.5835e-01, -4.4335e-01],\n",
       "          [ 1.8513e-01, -7.3155e-01,  2.1067e-01],\n",
       "          [-2.7477e-01,  1.9219e-01,  1.0249e-01]],\n",
       "\n",
       "         [[ 1.6879e+00, -1.7029e+00, -6.0471e-03],\n",
       "          [-2.5653e-01,  7.5084e-02,  8.4781e-01],\n",
       "          [-2.6594e-01, -3.6018e-01,  2.6335e-02]],\n",
       "\n",
       "         [[-2.2062e-01,  7.0717e-01, -3.0107e-01],\n",
       "          [-5.7842e-01,  2.2919e-01,  5.1473e-01],\n",
       "          [ 1.6060e-01, -4.3417e-01, -7.0878e-01]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.2652e-01, -1.3600e+00,  7.7400e-01],\n",
       "          [-1.0571e+00,  1.3203e+00, -1.0033e-02],\n",
       "          [-8.4564e-01,  9.1146e-01, -1.3745e+00]],\n",
       "\n",
       "         [[-5.4707e-01, -7.5527e-05, -1.2117e-01],\n",
       "          [-2.0086e+00, -9.2065e-01,  1.6823e-01],\n",
       "          [-1.3199e+00,  1.2664e+00,  4.9518e-01]],\n",
       "\n",
       "         [[-5.1424e-01, -2.2029e-01,  1.8616e+00],\n",
       "          [ 9.3599e-01,  3.8022e-01, -1.4155e+00],\n",
       "          [ 1.6296e+00,  1.0524e+00, -1.4841e-01]],\n",
       "\n",
       "         [[-5.4970e-01, -1.8790e-01, -1.2019e+00],\n",
       "          [-4.7079e-01,  7.6316e-01, -1.8076e+00],\n",
       "          [-3.1407e-01,  1.1376e-01,  1.0357e-01]],\n",
       "\n",
       "         [[-1.1789e+00, -1.1822e+00,  1.0892e+00],\n",
       "          [-1.2245e+00,  1.0087e+00, -4.8237e-01],\n",
       "          [ 1.0798e+00, -4.2108e-01, -1.1665e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 8.5655e-01, -1.7391e-02,  1.4486e+00],\n",
       "          [ 8.9220e-01, -2.2943e-01, -4.4967e-01],\n",
       "          [ 2.3372e-02,  1.9021e-01, -8.8175e-01]],\n",
       "\n",
       "         [[ 8.4194e-01, -3.9736e-01, -4.2303e-01],\n",
       "          [-5.4069e-01,  2.3102e-01, -6.9205e-01],\n",
       "          [ 1.3497e-01,  2.7666e+00, -5.3609e-02]],\n",
       "\n",
       "         [[-4.3400e-01, -1.6677e+00,  5.0222e-02],\n",
       "          [-1.1092e+00, -3.7556e-01,  1.5161e-01],\n",
       "          [-1.7310e+00,  1.5746e-01,  3.0452e-01]],\n",
       "\n",
       "         [[-1.2971e+00, -3.9231e-01, -1.8307e+00],\n",
       "          [ 1.5755e+00,  3.3056e-01, -1.7959e-01],\n",
       "          [-1.6344e-01,  1.1314e+00, -9.4166e-02]],\n",
       "\n",
       "         [[ 3.3082e-01,  1.5186e+00, -3.4617e-01],\n",
       "          [-1.0926e+00, -8.2450e-01,  1.4287e+00],\n",
       "          [ 9.1428e-02, -5.0233e-01,  9.7364e-01]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(B@A).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.11022302e-16, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.random.randn(2,5,3,3)\n",
    "B=np.random.randn(2,5,3,3)\n",
    "print((A@B).shape)\n",
    "A[0,2] @ B[0,2] - (A@B)[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch and broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 8.32667268e-17, 0.00000000e+00],\n",
       "       [1.38777878e-17, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.random.randn(2,1,3,3)\n",
    "B=np.random.randn(1,5,3,3)\n",
    "print((A@B).shape)\n",
    "A[1,0] @ B[0,2] - (A@B)[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-4.44089210e-16,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  2.22044605e-16,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fills in missing dimensions\n",
    "A=np.random.randn(2,1,3,3)\n",
    "B=np.random.randn(  5,3,3)\n",
    "print((A@B).shape)\n",
    "A[1,0] @ B[2] - (A@B)[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.random.randn(2,5,3,3)\n",
    "B=np.random.randn(3)\n",
    "print((A@B).shape)\n",
    "A[1,2] @ B - (A@B)[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch, Tensors, and Basic Linear Algebra\n",
    "\n",
    "Pytorch has many different components. Let's start with Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0461,  0.4024, -1.0115],\n",
      "        [ 0.2167, -0.6123,  0.5036],\n",
      "        [ 0.2310,  0.6931, -0.2669]])\n",
      "<class 'torch.Tensor'> torch.float32 torch.Size([3, 3]) torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "A = torch.randn(3,3)\n",
    "print(A)\n",
    "print(type(A), A.dtype, A.shape, A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNumpy = np.random.randn(1,3)\n",
    "# import from numpy array to a tensor\n",
    "B = torch.tensor(BNumpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0959,  1.1544, -0.9419],\n",
      "        [ 0.0747,  0.1398,  0.5733],\n",
      "        [ 0.0889,  1.4451, -0.1972]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# or Import \n",
    "B = torch.from_numpy(BNumpy)\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09592026  1.15443649 -0.94188834]\n",
      " [ 0.07468751  0.13976057  0.57325148]\n",
      " [ 0.08891608  1.44511099 -0.19721982]]\n"
     ]
    }
   ],
   "source": [
    "# export to numpy\n",
    "ANumpy = A.numpy()\n",
    "print(ANumpy+BNumpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear Algebra (in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0959,  1.1544, -0.9419],\n",
      "        [ 0.0747,  0.1398,  0.5733],\n",
      "        [ 0.0889,  1.4451, -0.1972]], dtype=torch.float64)\n",
      "tensor([[-0.0959,  1.1544, -0.9419],\n",
      "        [ 0.0747,  0.1398,  0.5733],\n",
      "        [ 0.0889,  1.4451, -0.1972]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Broadcasts a lot like numpy\n",
    "print(torch.add(A,B))\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7545, -2.2618], dtype=torch.float64)\n",
      "[ 0.75454611 -2.26182377]\n",
      "[ 2.61767537 -2.03552265 -1.1993764 ]\n",
      "tensor([ 2.6177, -2.0355, -1.1994], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "AN=np.random.randn(2,3)\n",
    "xN=np.random.randn(3)\n",
    "yN=np.random.randn(2)\n",
    "A=torch.tensor(AN)\n",
    "x=torch.tensor(xN)\n",
    "y=torch.tensor(yN)\n",
    "\n",
    "print(A@x)\n",
    "print(AN@xN)\n",
    "\n",
    "print(yN@AN)\n",
    "print(y@A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 3, 3)\n",
      "[[0.00000000e+00 2.77555756e-17 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "torch.Size([2, 5, 3, 3])\n",
      "tensor([[0.0000e+00, 2.7756e-17, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.7756e-17, 0.0000e+00, 0.0000e+00]], dtype=torch.float64)\n",
      "torch.Size([2, 5, 3, 3])\n",
      "tensor([[0.0000e+00, 2.7756e-17, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.7756e-17, 0.0000e+00, 0.0000e+00]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "AN=np.random.randn(2,1,3,3)\n",
    "BN=np.random.randn(1,5,3,3)\n",
    "A=torch.tensor(AN)\n",
    "B=torch.tensor(BN)\n",
    "print((AN@BN).shape)\n",
    "print(AN[1,0] @ BN[0,2] - (AN@BN)[1,2])\n",
    "\n",
    "print((A@B).shape)\n",
    "print(A[1,0] @ B[0,2] - (A@B)[1,2])\n",
    "\n",
    "print(torch.matmul(A,B).shape)\n",
    "print(A[1,0] @ B[0,2] - (torch.matmul(A,B))[1,2])\n",
    "# Interestingly, the difference may not be exactly zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 4)\n",
      "[ 0.00000000e+00  0.00000000e+00 -2.77555756e-17 -2.22044605e-16]\n",
      "torch.Size([2, 5, 4])\n",
      "tensor([0., 0., 0., 0.], dtype=torch.float64)\n",
      "torch.Size([2, 5, 4])\n",
      "tensor([0., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "AN=np.random.randn(2,5,4,3)\n",
    "BN=np.random.randn(3)\n",
    "A=torch.tensor(AN)\n",
    "B=torch.tensor(BN)\n",
    "print((AN@BN).shape)\n",
    "print(AN[1,2] @ BN - (AN@BN)[1,2])\n",
    "\n",
    "print((A@B).shape)\n",
    "print(A[1,2] @ B - (A@B)[1,2])\n",
    "\n",
    "print(torch.matmul(A,B).shape)\n",
    "print(A[1,2] @ B - torch.tensor(AN[1,2] @ BN ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -3.8745e-17,  9.5920e-17],\n",
       "        [-2.1308e-16,  1.0000e+00,  4.5501e-16],\n",
       "        [ 1.4665e-17, -1.6948e-18,  1.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AN=np.random.randn(2,5,3,3)\n",
    "A=torch.tensor(AN)\n",
    "B = torch.inverse(A)\n",
    "B[0,2]@A[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 3, 3]) torch.Size([2, 5, 3]) torch.Size([2, 5, 3, 3])\n",
      "numpy SVD gives V.T out: \n",
      " [[-1.11022302e-16 -2.22044605e-16  3.53883589e-16]\n",
      " [-2.22044605e-16  1.11022302e-16 -1.11022302e-16]\n",
      " [-4.44089210e-16  2.22044605e-16 -5.55111512e-17]]\n",
      "torch SVD is not the same! \n",
      " tensor([[ 0.0086,  0.0081, -0.2420],\n",
      "        [ 0.0069,  0.0110, -0.3672],\n",
      "        [-0.0112,  0.0114, -0.5281]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# This might not work on old version of Torch which didn't have batches for SVD\n",
    "AN=np.random.randn(2,5,3,3)\n",
    "A=torch.tensor(AN)\n",
    "U,D,V = torch.svd(A)\n",
    "UN,DN,VN = la.svd(A)\n",
    "\n",
    "print(U.shape,D.shape,V.shape)\n",
    "\n",
    "\n",
    "print(\"numpy SVD gives V.T out: \\n\", \n",
    "      UN[1,2] @ np.diag(DN[1,2]) @ VN[1,2] - AN[1,2])\n",
    "\n",
    "print(\"torch SVD is not the same! \\n\", \n",
    "      U[1,2] @ torch.diag(D[1,2]) @ V[1,2] - A[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch SVD gives V, not V^T \n",
      " tensor([[-1.1102e-16, -2.2204e-16,  3.5388e-16],\n",
      "        [-2.2204e-16,  1.1102e-16, -1.1102e-16],\n",
      "        [-4.4409e-16,  2.2204e-16, -5.5511e-17]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Why didn't this work like SVD???\n",
    "print(\"torch SVD gives V, not V^T \\n\", \n",
    "      U[1,2] @ torch.diag(D[1,2]) @ V[1,2].T - A[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(D.shape)\n",
    "# Not everything goes into batchs :/\n",
    "# torch.diag(D).shape\n",
    "# There are very similar duplications... :/\n",
    "torch.diag_embed(D).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-3f2e6f4264fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try to reconstruct A?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mAReconstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m@\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# OOOOOOPPPPPSSSSS.....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Try to reconstruct A?\n",
    "AReconstruct = U @ torch.diag_embed(D)  @ V.T\n",
    "# OOOOOOPPPPPSSSSS....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all operations go into batches as if they were matrices\n",
    "# Transpose may not be what you expect\n",
    "print(V.shape)\n",
    "print(V.T.shape)\n",
    "print(V.transpose(-1,-2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, here's the reconstruction of all the matrices in A\n",
    "AReconstruct = U @ torch.diag_embed(D)  @ V.transpose(-1,-2) \n",
    "print(AReconstruct- A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In place operations\n",
    "\n",
    "Usually end with \"_\". These update the same place in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2,3,3)\n",
    "B = torch.randn(2,1,3)\n",
    "print(\"A=\",A)\n",
    "C=A.add(B)\n",
    "print(\"A didn't change:\",A)\n",
    "C=A.add_(B)\n",
    "print(\"A changed:\", A)\n",
    "# C=B.add_(A) # B is too small. This won't work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "What does this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=2\n",
    "n1=3\n",
    "x=torch.randn(n0,n1)\n",
    "w=torch.randn(n0,n1)\n",
    "\n",
    "z = (x * w).sum()\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=torch.zeros(1)\n",
    "for j0 in range(n0):\n",
    "    for j1 in range(n1):\n",
    "        z2=z2+(x[j0,j1]*w[j0,j1])\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "What does this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=2\n",
    "n1=3\n",
    "n2=4\n",
    "x=torch.randn(n0,n1,n2)\n",
    "w=torch.randn(1,n1,1)\n",
    "y=torch.randn(1,1,n2)\n",
    "\n",
    "z = (x*w - y).sum(2).pow(2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=torch.zeros(1)\n",
    "for j0 in range(n0):\n",
    "    for j1 in range(n1):\n",
    "        tmpz = torch.zeros(1)\n",
    "        for j2 in range(n2):\n",
    "            tmpz = tmpz + (x[j0,j1,j2]*w[0,j1,0]-y[0,0,j2])\n",
    "        tmpz = tmpz **2\n",
    "        z2=z2+tmpz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "What does this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=7\n",
    "n1=4\n",
    "n2=4\n",
    "x=torch.randn(n2)\n",
    "A=torch.randn(n0,n1,n2)\n",
    "\n",
    "z = ((x@A@x) / (x@x) ).sum()\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=torch.zeros(1)\n",
    "for j0 in range(n0):\n",
    "    z2 = z2 + (x@A[j0]@x) / (x@x)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "Pytorch tracks computations that we do, and can compute gradients for us!\n",
    "\n",
    "In order to tell pytorch to keep track of computation in order to differentiate with respect to x, we need to x the \"requires_grad\" propertiy to \"True\".\n",
    "\n",
    "We compute some z=f(x) and then tell pytorch to differentiate z with respect to x.\n",
    "\n",
    "$\\nabla_x f$ will appear in x.grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=4\n",
    "n1=4\n",
    "x=torch.randn(n1)\n",
    "A=torch.randn(n0,n1)\n",
    "A = A.T @ A\n",
    "\n",
    "print(x.requires_grad)\n",
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)\n",
    "\n",
    "z = (x@A@x) \n",
    "\n",
    "print(z)\n",
    "print(x.grad)\n",
    "z.backward()\n",
    "print(\"auto gradient    :\", x.grad)\n",
    "\n",
    "print(\"analytic gradient:\", A@x + A.T@x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = (x@A@x) \n",
    "z2.backward()\n",
    "print(\"auto gradient    :\", x.grad, \"Why did the result change?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients are aggregated. Every time we compute the gradient, it is added to what is already in x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_() # initialize the gradient to zero\n",
    "z2 = (x@A@x) \n",
    "z2.backward()\n",
    "print(\"auto gradient    :\", x.grad, \"We reset the gradient and compute it again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "x0=x.detach().clone()\n",
    "for j0 in range(20):\n",
    "    x.requires_grad_(True)\n",
    "    #if ~(x.grad is None):\n",
    "    #    x.grad.zero_()\n",
    "    z2 = (x@A@x) \n",
    "    z2.backward()\n",
    "    mygrad = x.grad\n",
    "    print(j0, z2.detach().numpy(), x.detach().numpy())   # Detach creates a copy outside of the computation graph (try without it)\n",
    "    #print(\"   f(x)=\",z2)\n",
    "    #print(\"   check grad: \",mygrad , \"  diff:\",  mygrad - (A.T@x + A@x) )\n",
    "    #x = x.sub_(alpha*mygrad)\n",
    "    x = x.data.sub_(alpha*mygrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.data.sub_ performing calculation based on the data \n",
    "# x.sub_ is actually adding something or removing something from the computational graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x0.detach().clone()\n",
    "x1.requires_grad_(True)\n",
    "optimizer = optim.SGD([x1], lr=alpha)\n",
    "for j0 in range(20):\n",
    "    #print(j0,x1)\n",
    "    optimizer.zero_grad()\n",
    "    z2 = (x1@A@x1)    \n",
    "    print(j0, z2.detach().numpy(), x1.detach().numpy())\n",
    "    z2.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different algorithms: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=x0.detach().clone()\n",
    "x2.requires_grad_(True)\n",
    "optimizer = optim.Adam([x2], lr=alpha)\n",
    "for j0 in range(20):\n",
    "    #print(j0,x2)\n",
    "    optimizer.zero_grad()\n",
    "    z2 = (x2@A@x2) \n",
    "    print(j0, z2.detach().numpy(), x2.detach().numpy())\n",
    "    z2.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian (**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=4\n",
    "n1=4\n",
    "x=torch.randn(n1)\n",
    "A=torch.randn(n0,n1)\n",
    "A = A.T @ A\n",
    "\n",
    "print(x.requires_grad)\n",
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)\n",
    "\n",
    "z = (x@A@x) \n",
    "\n",
    "print(z)\n",
    "\n",
    "# this is like x.grad, but it tracks the computations it did, so we can treat it as variables.\n",
    "# g is going to be a list of gradients. Since we only have one tensor (x), g[0] is out gradient with respect to x.\n",
    "g = torch.autograd.grad(z, x, retain_graph=True, create_graph=True) \n",
    "print(\"g:                \",g)\n",
    "print(\"gradient:         \",g[0])\n",
    "print(\"analytic gradient:\", A@x + A.T@x)\n",
    "\n",
    "# autograd works only on scalars, so we need to loop over the entries of the gradient\n",
    "H_list = []\n",
    "for j1 in range(len(g[0])):\n",
    "    g2 = torch.autograd.grad(g[0][j1], x, retain_graph=True, create_graph=True)[0]\n",
    "    H_list.append( g2 )\n",
    "# Python shortcut for thisL\n",
    "# H_list = [torch.autograd.grad(g[0][j1], x, retain_graph=True, create_graph=True)[0] for j1 in range(n1)]\n",
    "\n",
    "\n",
    "H = torch.stack(H_list)\n",
    "\n",
    "print(\"Our Hessian:     \\n\", H.detach().numpy())\n",
    "print(\"analytic Hessian:\\n\", (A + A.T).detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devices (*)\n",
    "\n",
    "If you have a GPU, torch can use your GPU. It can get a little tricky to move things when you need to be explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you have a GPU on your computer?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can fail if A is on CPU and B on GPU\n",
    "A = torch.randn(BNumpy.shape)\n",
    "B = torch.tensor(BNumpy, dtype=torch.float, device=device)\n",
    "A+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can fail on GPU. Types on GPU are more tricky.\n",
    "A = torch.randn(BNumpy.shape,device=device)\n",
    "B = torch.tensor(BNumpy,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.randn(3,3,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=A.to(device)\n",
    "print(B.device)\n",
    "print(A.device)\n",
    "\n",
    "A=B.cpu()\n",
    "print(B.device)\n",
    "print(A.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a variable with the same properties..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=torch.ones_like(A)\n",
    "print(A.shape, A.device, A.dtype)\n",
    "print(B.shape, B.device, B.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Any code you submit must run properly on CPU***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Collab (**)\n",
    "\n",
    "https://colab.research.google.com/\n",
    "\n",
    "Choosing GPU vs. CPU\n",
    "\n",
    "runtime -> change runtime type\n",
    "\n",
    "***Unless we say otherwise, you can't submit collab files.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Do you have a GPU on your computer?\n",
    "torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "A = torch.randn(3,3)\n",
    "print(A)\n",
    "print(type(A), A.dtype, A.shape, A.size())\n",
    "BNumpy = np.random.randn(1,3)\n",
    "# import to a tensor\n",
    "B = torch.tensor(BNumpy)\n",
    "\n",
    "# this can fail if A is on CPU and B on GPU\n",
    "B = torch.tensor(BNumpy, dtype=torch.float, device=device)\n",
    "#B = torch.tensor(BNumpy, dtype=torch.float, device=\"cpu\")\n",
    "# this can fail if A is on CPU and B on GPU\n",
    "A+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving devices\n",
    "A = torch.randn(2,2)\n",
    "B=A.to(device)\n",
    "print(B.device)\n",
    "print(A.device)\n",
    "\n",
    "A=B.cpu()\n",
    "print(B.device)\n",
    "print(A.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=100\n",
    "n1=200\n",
    "n2=200\n",
    "device0 = \"cpu\"\n",
    "#device0=device\n",
    "A=torch.rand(n0,n1,n2,device=device0)\n",
    "x=torch.rand(n2,device=device0)\n",
    "z2=torch.zeros(1)\n",
    "torch.cuda.synchronize()     # In order to get timing right, we tell the gpu to finish what it is doing. \n",
    "t0=time.time()\n",
    "for j1 in range(100):\n",
    "  z=(x@A@x).sum()\n",
    "  z2=z2+z.to(\"cpu\")\n",
    "  torch.cuda.synchronize()\n",
    "t1=time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=100\n",
    "n1=200\n",
    "n2=200\n",
    "device0 = \"cpu\"\n",
    "device0=device\n",
    "A=torch.rand(n0,n1,n2,device=device0)\n",
    "x=torch.rand(n2,device=device0)\n",
    "z2=torch.zeros(1)\n",
    "torch.cuda.synchronize()\n",
    "t0=time.time()\n",
    "for j1 in range(100):\n",
    "  z=(A.inverse()).sum()\n",
    "  z2=z2+z.to(\"cpu\")\n",
    "  torch.cuda.synchronize()\n",
    "t1=time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python functions and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc (B,v):\n",
    "    return B@v\n",
    "\n",
    "n0=4\n",
    "n1=n0\n",
    "x=np.random.randn(n1)\n",
    "A=np.random.randn(n0,n1)\n",
    "myfunc(A,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myclass():\n",
    "    def __init__(self):\n",
    "        self.a=[1,2,3]\n",
    "    def mycall(self):\n",
    "        return self.a\n",
    "    def mycall2(self,b):\n",
    "        return(self.a*b)\n",
    "    \n",
    "myobj = myclass()\n",
    "print(myobj.a)\n",
    "print(myobj.mycall())\n",
    "print(myobj.mycall2(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But....\n",
    "myclass.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create classes that inherit functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myclass2(myclass):\n",
    "    def __init__(self,m):\n",
    "        super(myclass2, self).__init__()  # Run the inherited init\n",
    "        self.m = m\n",
    "    def mycall3(self,b):\n",
    "        return(self.m*b)\n",
    "    \n",
    "myobj2 = myclass2(np.array([1,2,3]))\n",
    "print(myobj2.a)\n",
    "print(myobj2.mycall())\n",
    "print(myobj2.mycall2(2))\n",
    "print(myobj2.mycall3(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and networks in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(Net, self).__init__()\n",
    "        self.myx = nn.Parameter( torch.randn(d) )\n",
    "    def forward(self, A):\n",
    "        return self.myx@A@self.myx\n",
    "    \n",
    "\n",
    "n0=4\n",
    "n1=n0\n",
    "mynet = Net(n1)\n",
    "x=torch.randn(n1)\n",
    "A=torch.randn(n0,n1)\n",
    "A = A.T @ A\n",
    "\n",
    "loss = mynet(A)\n",
    "print(mynet.myx)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mynet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "loss.backward()\n",
    "for f in mynet.parameters():\n",
    "    print(f)\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = optim.SGD(mynet.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "for j1 in range (20):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = mynet(A)\n",
    "    loss = output\n",
    "    loss.backward()\n",
    "    print(j1,loss.detach().numpy(), mynet.myx.detach().numpy())\n",
    "    optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work? (**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
