{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Dimensional Arrays and a Little More Broadcasting\n",
    "\n",
    "\n",
    "A high dimensional array has multiple indices: $A[i_1,i_2,...,i_k]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "A = np.random.randn(2,3,4,3)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A[0,2,3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have operations such as suming one of the dimensions. Suppose we sum the third dimension:\n",
    "\n",
    "\\begin{equation}\n",
    "    B[i_1,i_2,i_3,i_4] = \\sum_{i_3} A[i_1,i_2,i_3,i_4]\n",
    "\\end{equation}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "    B[i_1,i_2,i_4] = \\sum_{i_3} A[i_1,i_2,i_3,i_4]\n",
    "\\end{equation}\n",
    "\n",
    "Depending on if we \"keep the dimension\" or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.sum(axis=2,keepdims=True).shape)\n",
    "print(A.sum(axis=2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can do element wise operations on ndarrays or on pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "A = np.random.rand(2,3,4,2)\n",
    "Z = np.sqrt(A)\n",
    "print(Z[1,2,2,1],np.sqrt(A[1,2,2,1]))\n",
    "\n",
    "Z = A**2\n",
    "print(Z[1,2,2,1],(A[1,2,2,1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(2,3,4,2)\n",
    "B = np.random.randn(2,3,4,2)\n",
    "\n",
    "Z = A*B\n",
    "print(Z[1,2,2,1],(A[1,2,2,1]*B[1,2,2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why this way?\n",
    "A.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.reshape(-1,4).T.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Broadcasting examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.random.randn(d)\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = np.ones((d,d))\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A2 * A1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.ones((d+2,d+1,d))\n",
    "print(B*A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A4 = A1.reshape(1,-1)\n",
    "print(A4)\n",
    "print(A4.shape)\n",
    "print(\" \")\n",
    "print(B*A4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A5 = A1.reshape(-1,1)\n",
    "print(A5)\n",
    "print(\" \")\n",
    "print(B*A5)\n",
    "# OOOOOPPPPPPSSSSS....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A6 = np.random.randn(d+2,1,d)\n",
    "print(A6.shape)\n",
    "print(B.shape)\n",
    "print(A6*B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most operations, when broadcasting,\n",
    "- dimension of length 1 behave as if they were filled with copies.\n",
    "- an array with fewer dimensions behave as if we added \"1\" dimensions to the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.random.randn(2,5,3,3)\n",
    "# what does this mean for a n-d array????\n",
    "B = la.inv(A)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treated as an array of matrices\n",
    "B[0,2]@A[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.random.randn(2,5,3,3)\n",
    "B=np.random.randn(2,5,3,3)\n",
    "print((A@B).shape)\n",
    "A[0,2] @ B[0,2] - (A@B)[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch and broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.random.randn(2,1,3,3)\n",
    "B=np.random.randn(1,5,3,3)\n",
    "print((A@B).shape)\n",
    "A[1,0] @ B[0,2] - (A@B)[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fills in missing dimensions\n",
    "A=np.random.randn(2,1,3,3)\n",
    "B=np.random.randn(  5,3,3)\n",
    "print((A@B).shape)\n",
    "A[1,0] @ B[2] - (A@B)[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.random.randn(2,5,3,3)\n",
    "B=np.random.randn(3)\n",
    "print((A@B).shape)\n",
    "A[1,2] @ B - (A@B)[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch, Tensors, and Basic Linear Algebra\n",
    "\n",
    "Pytorch has many different components. Let's start with Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "A = torch.randn(3,3)\n",
    "print(A)\n",
    "print(type(A), A.dtype, A.shape, A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNumpy = np.random.randn(1,3)\n",
    "# import from numpy array to a tensor\n",
    "B = torch.tensor(BNumpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or Import \n",
    "B = torch.from_numpy(BNumpy)\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to numpy\n",
    "ANumpy = A.numpy()\n",
    "print(ANumpy+BNumpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear Algebra (in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasts a lot like numpy\n",
    "print(torch.add(A,B))\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AN=np.random.randn(2,3)\n",
    "xN=np.random.randn(3)\n",
    "yN=np.random.randn(2)\n",
    "A=torch.tensor(AN)\n",
    "x=torch.tensor(xN)\n",
    "y=torch.tensor(yN)\n",
    "\n",
    "print(A@x)\n",
    "print(AN@xN)\n",
    "\n",
    "print(yN@AN)\n",
    "print(y@A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AN=np.random.randn(2,1,3,3)\n",
    "BN=np.random.randn(1,5,3,3)\n",
    "A=torch.tensor(AN)\n",
    "B=torch.tensor(BN)\n",
    "print((AN@BN).shape)\n",
    "print(AN[1,0] @ BN[0,2] - (AN@BN)[1,2])\n",
    "\n",
    "print((A@B).shape)\n",
    "print(A[1,0] @ B[0,2] - (A@B)[1,2])\n",
    "\n",
    "print(torch.matmul(A,B).shape)\n",
    "print(A[1,0] @ B[0,2] - (torch.matmul(A,B))[1,2])\n",
    "# Interestingly, the difference may not be exactly zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AN=np.random.randn(2,5,4,3)\n",
    "BN=np.random.randn(3)\n",
    "A=torch.tensor(AN)\n",
    "B=torch.tensor(BN)\n",
    "print((AN@BN).shape)\n",
    "print(AN[1,2] @ BN - (AN@BN)[1,2])\n",
    "\n",
    "print((A@B).shape)\n",
    "print(A[1,2] @ B - (A@B)[1,2])\n",
    "\n",
    "print(torch.matmul(A,B).shape)\n",
    "print(A[1,2] @ B - torch.tensor(AN[1,2] @ BN ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AN=np.random.randn(2,5,3,3)\n",
    "A=torch.tensor(AN)\n",
    "B = torch.inverse(A)\n",
    "B[0,2]@A[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might not work on old version of Torch which didn't have batches for SVD\n",
    "AN=np.random.randn(2,5,3,3)\n",
    "A=torch.tensor(AN)\n",
    "U,D,V = torch.svd(A)\n",
    "UN,DN,VN = la.svd(A)\n",
    "\n",
    "print(U.shape,D.shape,V.shape)\n",
    "\n",
    "\n",
    "print(\"numpy SVD gives V.T out: \\n\", \n",
    "      UN[1,2] @ np.diag(DN[1,2]) @ VN[1,2] - AN[1,2])\n",
    "\n",
    "print(\"torch SVD is not the same! \\n\", \n",
    "      U[1,2] @ torch.diag(D[1,2]) @ V[1,2] - A[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why didn't this work like SVD???\n",
    "print(\"torch SVD gives V, not V^T \\n\", \n",
    "      U[1,2] @ torch.diag(D[1,2]) @ V[1,2].T - A[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D.shape)\n",
    "# Not everything goes into batchs :/\n",
    "# torch.diag(D).shape\n",
    "# There are very similar duplications... :/\n",
    "torch.diag_embed(D).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to reconstruct A?\n",
    "AReconstruct = U @ torch.diag_embed(D)  @ V.T\n",
    "# OOOOOOPPPPPSSSSS....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all operations go into batches as if they were matrices\n",
    "# Transpose may not be what you expect\n",
    "print(V.shape)\n",
    "print(V.T.shape)\n",
    "print(V.transpose(-1,-2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, here's the reconstruction of all the matrices in A\n",
    "AReconstruct = U @ torch.diag_embed(D)  @ V.transpose(-1,-2) \n",
    "print(AReconstruct- A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In place operations\n",
    "\n",
    "Usually end with \"_\". These update the same place in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2,3,3)\n",
    "B = torch.randn(2,1,3)\n",
    "print(\"A=\",A)\n",
    "C=A.add(B)\n",
    "print(\"A didn't change:\",A)\n",
    "C=A.add_(B)\n",
    "print(\"A changed:\", A)\n",
    "# C=B.add_(A) # B is too small. This won't work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "What does this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=2\n",
    "n1=3\n",
    "x=torch.randn(n0,n1)\n",
    "w=torch.randn(n0,n1)\n",
    "\n",
    "z = (x * w).sum()\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=torch.zeros(1)\n",
    "for j0 in range(n0):\n",
    "    for j1 in range(n1):\n",
    "        z2=z2+(x[j0,j1]*w[j0,j1])\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "What does this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=2\n",
    "n1=3\n",
    "n2=4\n",
    "x=torch.randn(n0,n1,n2)\n",
    "w=torch.randn(1,n1,1)\n",
    "y=torch.randn(1,1,n2)\n",
    "\n",
    "z = (x*w - y).sum(2).pow(2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=torch.zeros(1)\n",
    "for j0 in range(n0):\n",
    "    for j1 in range(n1):\n",
    "        tmpz = torch.zeros(1)\n",
    "        for j2 in range(n2):\n",
    "            tmpz = tmpz + (x[j0,j1,j2]*w[0,j1,0]-y[0,0,j2])\n",
    "        tmpz = tmpz **2\n",
    "        z2=z2+tmpz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "What does this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=7\n",
    "n1=4\n",
    "n2=4\n",
    "x=torch.randn(n2)\n",
    "A=torch.randn(n0,n1,n2)\n",
    "\n",
    "z = ((x@A@x) / (x@x) ).sum()\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2=torch.zeros(1)\n",
    "for j0 in range(n0):\n",
    "    z2 = z2 + (x@A[j0]@x) / (x@x)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "Pytorch tracks computations that we do, and can compute gradients for us!\n",
    "\n",
    "In order to tell pytorch to keep track of computation in order to differentiate with respect to x, we need to x the \"requires_grad\" propertiy to \"True\".\n",
    "\n",
    "We compute some z=f(x) and then tell pytorch to differentiate z with respect to x.\n",
    "\n",
    "$\\nabla_x f$ will appear in x.grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=4\n",
    "n1=4\n",
    "x=torch.randn(n1)\n",
    "A=torch.randn(n0,n1)\n",
    "A = A.T @ A\n",
    "\n",
    "print(x.requires_grad)\n",
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)\n",
    "\n",
    "z = (x@A@x) \n",
    "\n",
    "print(z)\n",
    "print(x.grad)\n",
    "z.backward()\n",
    "print(\"auto gradient    :\", x.grad)\n",
    "\n",
    "print(\"analytic gradient:\", A@x + A.T@x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = (x@A@x) \n",
    "z2.backward()\n",
    "print(\"auto gradient    :\", x.grad, \"Why did the result change?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients are aggregated. Every time we compute the gradient, it is added to what is already in x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_() # initialize the gradient to zero\n",
    "z2 = (x@A@x) \n",
    "z2.backward()\n",
    "print(\"auto gradient    :\", x.grad, \"We reset the gradient and compute it again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "x0=x.detach().clone()\n",
    "for j0 in range(20):\n",
    "    x.requires_grad_(True)\n",
    "    #if ~(x.grad is None):\n",
    "    #    x.grad.zero_()\n",
    "    z2 = (x@A@x) \n",
    "    z2.backward()\n",
    "    mygrad = x.grad\n",
    "    print(j0, z2.detach().numpy(), x.detach().numpy())   # Detach creates a copy outside of the computation graph (try without it)\n",
    "    #print(\"   f(x)=\",z2)\n",
    "    #print(\"   check grad: \",mygrad , \"  diff:\",  mygrad - (A.T@x + A@x) )\n",
    "    #x = x.sub_(alpha*mygrad)\n",
    "    x = x.data.sub_(alpha*mygrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x0.detach().clone()\n",
    "x1.requires_grad_(True)\n",
    "optimizer = optim.SGD([x1], lr=alpha)\n",
    "for j0 in range(20):\n",
    "    #print(j0,x1)\n",
    "    optimizer.zero_grad()\n",
    "    z2 = (x1@A@x1)    \n",
    "    print(j0, z2.detach().numpy(), x1.detach().numpy())\n",
    "    z2.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different algorithms: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=x0.detach().clone()\n",
    "x2.requires_grad_(True)\n",
    "optimizer = optim.Adam([x2], lr=alpha)\n",
    "for j0 in range(20):\n",
    "    #print(j0,x2)\n",
    "    optimizer.zero_grad()\n",
    "    z2 = (x2@A@x2) \n",
    "    print(j0, z2.detach().numpy(), x2.detach().numpy())\n",
    "    z2.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian (**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=4\n",
    "n1=4\n",
    "x=torch.randn(n1)\n",
    "A=torch.randn(n0,n1)\n",
    "A = A.T @ A\n",
    "\n",
    "print(x.requires_grad)\n",
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)\n",
    "\n",
    "z = (x@A@x) \n",
    "\n",
    "print(z)\n",
    "\n",
    "# this is like x.grad, but it tracks the computations it did, so we can treat it as variables.\n",
    "# g is going to be a list of gradients. Since we only have one tensor (x), g[0] is out gradient with respect to x.\n",
    "g = torch.autograd.grad(z, x, retain_graph=True, create_graph=True) \n",
    "print(\"g:                \",g)\n",
    "print(\"gradient:         \",g[0])\n",
    "print(\"analytic gradient:\", A@x + A.T@x)\n",
    "\n",
    "# autograd works only on scalars, so we need to loop over the entries of the gradient\n",
    "H_list = []\n",
    "for j1 in range(len(g[0])):\n",
    "    g2 = torch.autograd.grad(g[0][j1], x, retain_graph=True, create_graph=True)[0]\n",
    "    H_list.append( g2 )\n",
    "# Python shortcut for thisL\n",
    "# H_list = [torch.autograd.grad(g[0][j1], x, retain_graph=True, create_graph=True)[0] for j1 in range(n1)]\n",
    "\n",
    "\n",
    "H = torch.stack(H_list)\n",
    "\n",
    "print(\"Our Hessian:     \\n\", H.detach().numpy())\n",
    "print(\"analytic Hessian:\\n\", (A + A.T).detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devices (*)\n",
    "\n",
    "If you have a GPU, torch can use your GPU. It can get a little tricky to move things when you need to be explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you have a GPU on your computer?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can fail if A is on CPU and B on GPU\n",
    "A = torch.randn(BNumpy.shape)\n",
    "B = torch.tensor(BNumpy, dtype=torch.float, device=device)\n",
    "A+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can fail on GPU. Types on GPU are more tricky.\n",
    "A = torch.randn(BNumpy.shape,device=device)\n",
    "B = torch.tensor(BNumpy,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.randn(3,3,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=A.to(device)\n",
    "print(B.device)\n",
    "print(A.device)\n",
    "\n",
    "A=B.cpu()\n",
    "print(B.device)\n",
    "print(A.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a variable with the same properties..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=torch.ones_like(A)\n",
    "print(A.shape, A.device, A.dtype)\n",
    "print(B.shape, B.device, B.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Any code you submit must run properly on CPU***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Collab (**)\n",
    "\n",
    "https://colab.research.google.com/\n",
    "\n",
    "Choosing GPU vs. CPU\n",
    "\n",
    "runtime -> change runtime type\n",
    "\n",
    "***Unless we say otherwise, you can't submit collab files.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Do you have a GPU on your computer?\n",
    "torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "A = torch.randn(3,3)\n",
    "print(A)\n",
    "print(type(A), A.dtype, A.shape, A.size())\n",
    "BNumpy = np.random.randn(1,3)\n",
    "# import to a tensor\n",
    "B = torch.tensor(BNumpy)\n",
    "\n",
    "# this can fail if A is on CPU and B on GPU\n",
    "B = torch.tensor(BNumpy, dtype=torch.float, device=device)\n",
    "#B = torch.tensor(BNumpy, dtype=torch.float, device=\"cpu\")\n",
    "# this can fail if A is on CPU and B on GPU\n",
    "A+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving devices\n",
    "A = torch.randn(2,2)\n",
    "B=A.to(device)\n",
    "print(B.device)\n",
    "print(A.device)\n",
    "\n",
    "A=B.cpu()\n",
    "print(B.device)\n",
    "print(A.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=100\n",
    "n1=200\n",
    "n2=200\n",
    "device0 = \"cpu\"\n",
    "#device0=device\n",
    "A=torch.rand(n0,n1,n2,device=device0)\n",
    "x=torch.rand(n2,device=device0)\n",
    "z2=torch.zeros(1)\n",
    "torch.cuda.synchronize()     # In order to get timing right, we tell the gpu to finish what it is doing. \n",
    "t0=time.time()\n",
    "for j1 in range(100):\n",
    "  z=(x@A@x).sum()\n",
    "  z2=z2+z.to(\"cpu\")\n",
    "  torch.cuda.synchronize()\n",
    "t1=time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0=100\n",
    "n1=200\n",
    "n2=200\n",
    "device0 = \"cpu\"\n",
    "device0=device\n",
    "A=torch.rand(n0,n1,n2,device=device0)\n",
    "x=torch.rand(n2,device=device0)\n",
    "z2=torch.zeros(1)\n",
    "torch.cuda.synchronize()\n",
    "t0=time.time()\n",
    "for j1 in range(100):\n",
    "  z=(A.inverse()).sum()\n",
    "  z2=z2+z.to(\"cpu\")\n",
    "  torch.cuda.synchronize()\n",
    "t1=time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python functions and objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc (B,v):\n",
    "    return B@v\n",
    "\n",
    "n0=4\n",
    "n1=n0\n",
    "x=np.random.randn(n1)\n",
    "A=np.random.randn(n0,n1)\n",
    "myfunc(A,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myclass():\n",
    "    def __init__(self):\n",
    "        self.a=[1,2,3]\n",
    "    def mycall(self):\n",
    "        return self.a\n",
    "    def mycall2(self,b):\n",
    "        return(self.a*b)\n",
    "    \n",
    "myobj = myclass()\n",
    "print(myobj.a)\n",
    "print(myobj.mycall())\n",
    "print(myobj.mycall2(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But....\n",
    "myclass.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create classes that inherit functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myclass2(myclass):\n",
    "    def __init__(self,m):\n",
    "        super(myclass2, self).__init__()  # Run the inherited init\n",
    "        self.m = m\n",
    "    def mycall3(self,b):\n",
    "        return(self.m*b)\n",
    "    \n",
    "myobj2 = myclass2(np.array([1,2,3]))\n",
    "print(myobj2.a)\n",
    "print(myobj2.mycall())\n",
    "print(myobj2.mycall2(2))\n",
    "print(myobj2.mycall3(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and networks in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(Net, self).__init__()\n",
    "        self.myx = nn.Parameter( torch.randn(d) )\n",
    "    def forward(self, A):\n",
    "        return self.myx@A@self.myx\n",
    "    \n",
    "\n",
    "n0=4\n",
    "n1=n0\n",
    "mynet = Net(n1)\n",
    "x=torch.randn(n1)\n",
    "A=torch.randn(n0,n1)\n",
    "A = A.T @ A\n",
    "\n",
    "loss = mynet(A)\n",
    "print(mynet.myx)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mynet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "loss.backward()\n",
    "for f in mynet.parameters():\n",
    "    print(f)\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = optim.SGD(mynet.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "for j1 in range (20):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = mynet(A)\n",
    "    loss = output\n",
    "    loss.backward()\n",
    "    print(j1,loss.detach().numpy(), mynet.myx.detach().numpy())\n",
    "    optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work? (**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
