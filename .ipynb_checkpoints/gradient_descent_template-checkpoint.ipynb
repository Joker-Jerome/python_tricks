{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sum((sigmoid(A.dot(x))-b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at zero vector, on this particular set of test data, f should give 2.\n",
    "# run the next line to make sure it does\n",
    "x = np.zeros((d,1))\n",
    "assert f(x) == 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally, you should keep our test code. \n",
    "# For various reasons, == should actually work here. \n",
    "# But, we allow you to replace this condition with something like\n",
    "assert np.abs(f(x)-2.0) < 10e-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(z):\n",
    "    return 1./(np.exp(-z/2)+np.exp(z/2))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that it is equal = 0.25\n",
    "sigmoid_deriv(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check it at a random point.  \n",
    "Confirm that the values displayed on the next two lines are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13425617])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(1)\n",
    "delta = .0001\n",
    "(sigmoid(x+delta)-sigmoid(x-delta))/(2*delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13425617])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_deriv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that when you apply sigmoid_deriv to a vector, you get the vector of derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661193 0.10499359]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 1, 2])\n",
    "print(sigmoid_deriv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.shape == sigmoid_deriv(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a function that computes the gradient of f.  Call it grad_f.\n",
    "Put it in the following cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that the gradient of objective is\n",
    "$$\n",
    "\\nabla f(x) = 2 \\sum_{i=1}^n (\\sigma(a_i^T x) - b_i)\\sigma^{'}(a_i^T x) a_i\n",
    "$$\n",
    "\n",
    "Also recall that $a_i^T$ are the rows of the matrix $A$. If we let $\\epsilon_i = (\\sigma(a_i^T x) - b_i)\\sigma^{'}(a_i^T x)$, then we can simplify the above to\n",
    "$$\n",
    "2 A^T \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(x):\n",
    "    e=(sigmoid(A.dot(x))-b)*sigmoid_deriv(A.dot(x))\n",
    "    return 2*A.T.dot(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that your computation is correct on random vectors.\n",
    "The numbers printed in the next line should be close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.825378298571991e-07\n",
      "[[-3.82537776e-07]]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.random.randn(d,1)\n",
    "r = np.random.randn(d,1)/10**6\n",
    "print(f(x0+r) - f(x0))\n",
    "print(grad_f(x0).T @ (r) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or write a test like the one we did in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed :  62817\n"
     ]
    }
   ],
   "source": [
    "hash_object = hashlib.md5(netid.encode('utf-8'))\n",
    "hex_hash = hash_object.hexdigest()\n",
    "seed = int(hex_hash[0:4],16)\n",
    "print(\"seed : \", seed)\n",
    "np.random.seed(int(hex_hash[0:4],16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "d = 10\n",
    "\n",
    "A = np.random.randn(n,d)\n",
    "b = np.round(np.random.rand(n,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell.  It should not produce an error.\n",
    "We will want to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.184831753877276\n"
     ]
    }
   ],
   "source": [
    "assert A.shape == (30,10)\n",
    "print(np.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Descent Optimization\n",
    "Write some code to do gradient descent.\n",
    "Find a vector xx for which the norm of the gradient of f at xx is at most 10^(-6).\n",
    "\n",
    "**Please implement step size control**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad_descent(x0, f, g, tol=10**(-6),step_size=50,max_iters=100000):\n",
    "\n",
    "    f0 = f(x0)  # initial value\n",
    "    g0 = g(x0)  # initial gradient\n",
    "    \n",
    "    dbg_iters_actual  =0               # I'm going to use this to log the actual number of tryes I had, including things internal step size control\n",
    "    dbg_step_cond_met =0               # debugging information I like to collect.\n",
    "    for iters in range(max_iters):     # I only use for loop to avoid getting stuck somewhere. \n",
    "        if np.linalg.norm(g0)<tol:     # Stop condition based on gradient norm as I was asked to do. I would often check how much progress I made in recent iterations.\n",
    "            break\n",
    "        if iters%10000 ==0:             # Once a while print some information. I like to do this when I tests.             \n",
    "            # I do this at each iteration when I develop the code.\n",
    "            # Normally I would disable this in code I give out, but I would usually keep something in code I work with            \n",
    "            #f0 = f(x0)\n",
    "            #g0 = g(x0)\n",
    "            print(\"iteration information: \",iters , dbg_iters_actual, f0, np.linalg.norm(g0)) \n",
    "        tstep_size = step_size    # initialize step size to the same initial value at each iteration        \n",
    "        dbg_step_cond_met = -1    # I want to know if the step size worked.\n",
    "        for j1 in range(200):     # Step size control iterations. Try smaller steps until we get this right.\n",
    "            dbg_iters_actual = dbg_iters_actual+1\n",
    "            xnew = x0-tstep_size*g0\n",
    "            fnew = f(xnew)\n",
    "            if (j1>10):            \n",
    "                # I really want to know what's happening here. \n",
    "                # Again, when I develop I print everyhitng!\n",
    "                # I do this at each iteration when I develop the code.\n",
    "                # Normally I would disable this in code I give out, but I would usually keep something in code I work with            \n",
    "                print(\"     step size control information: \",iters , dbg_iters_actual, \"\\t\", f0, np.linalg.norm(g0), j1, tstep_size, fnew)             \n",
    "            if (fnew < f0): # If the step improves things\n",
    "                dbg_step_cond_met = j1  # Ok, this worked at the j1-th try\n",
    "                f0 = fnew   # the best function value\n",
    "                x0 = xnew   # the corresponding vector\n",
    "                g0 = g(x0)  # and, its gradient\n",
    "                break       # Stop if we hit the right point\n",
    "            else:           # try again with smaller step\n",
    "                tstep_size = tstep_size/2.0\n",
    "                continue\n",
    "            print(\"ERROR: step control is stuck\")  # Ha. We're not supposed to be in this state!\n",
    "        if dbg_step_cond_met<0:                    # Ha. We're not supposed to be in this state!\n",
    "            print(\"ERROR: step control didn't work, step still too large?\")  \n",
    "    \n",
    "    print(\"done:\", iters, dbg_iters_actual, \"\\t loss:\", f(x0), \"\\t   grad norm:\", np.linalg.norm(g(x0)))\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration information:  0 0 12.247870539709655 2.3767478991885196\n",
      "done: 2 2 \t loss: 7.0 \t   grad norm: 5.500770241481688e-17\n",
      "time: 0.0010654926300048828\n",
      "loss: 7.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "xxi = np.random.randn(d,1)\n",
    "t0=time.time()\n",
    "xx = grad_descent(xxi, f, grad_f, tol=10**(-6),step_size=100,max_iters=100)\n",
    "t1=time.time()\n",
    "print(\"time:\",t1-t0)\n",
    "print(\"loss:\", f(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that yout solutions satifies the requirement for xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient at xx is small!\n"
     ]
    }
   ],
   "source": [
    "if np.linalg.norm(grad_f(xx)) < 1e-6:\n",
    "    print(\"gradient at xx is small!\")\n",
    "else:\n",
    "    print (\"xx failed gradient test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the loss for your solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "print(f(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to run your optimization with a different starting point, and call your new solution xx2.\n",
    "Does it satisfy the requirement above? What is the loss for that solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration information:  0 0 10.198744507199393 1.3815220545796596\n",
      "iteration information:  10000 12276 1.0000296729449263 5.593211668198365e-06\n",
      "iteration information:  20000 22276 1.0000144388213843 2.723290525486981e-06\n",
      "iteration information:  30000 32276 1.0000095381621974 1.799450329931623e-06\n",
      "iteration information:  40000 42276 1.0000071204678045 1.343539172137822e-06\n",
      "iteration information:  50000 52276 1.0000056802920478 1.071908476478321e-06\n",
      "done: 53557 55833 \t loss: 1.000005299014613 \t   grad norm: 9.999886444326768e-07\n",
      "time: 2.9832687377929688\n",
      "loss: 1.000005299014613\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xx2i = np.random.randn(d,1)\n",
    "t0=time.time()\n",
    "xx2 = grad_descent(xx2i, f, grad_f, tol=10**(-6),step_size=100,max_iters=200000)\n",
    "t1=time.time()\n",
    "print(\"time:\",t1-t0)\n",
    "print(\"loss:\", f(xx2))\n",
    "# Intereting, this point happens to take much longer, but loss happens to be much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the problem is convex or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the function we study here is NOT exactly the logistic regression. We know that logistic regression is convex, but we don't know if this problem is convex. We will try to show next that it is not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== TRIAL  0\n",
      "iteration information:  0 0 12.247870539709655 2.3767478991885196\n",
      "done: 7070 7072 \t loss: 7.000000359167062 \t   grad norm: 9.99997276816361e-07\n",
      "iteration information:  0 0 9.696461243796197 1.5400106651309375\n",
      "iteration information:  10000 11576 1.000058927362518 1.109847533649669e-05\n",
      "iteration information:  20000 21576 1.0000287980091662 5.428451017801271e-06\n",
      "iteration information:  30000 31576 1.0000190481747326 3.5919142359707857e-06\n",
      "iteration information:  40000 41576 1.000014228357639 2.683622452019362e-06\n",
      "iteration information:  50000 51576 1.0000113543246432 2.1418637093067053e-06\n",
      "iteration information:  60000 61576 1.0000094458493414 1.7820445300081961e-06\n",
      "iteration information:  70000 71576 1.000008086401247 1.52570048974852e-06\n",
      "iteration information:  80000 81576 1.0000070688929852 1.3338123962089644e-06\n",
      "iteration information:  90000 91576 1.0000062787448825 1.1847874791750144e-06\n",
      "iteration information:  100000 101576 1.0000056474218038 1.065708333862019e-06\n",
      "done: 106538 108114 \t loss: 1.0000052990420556 \t   grad norm: 9.999938209698783e-07\n",
      "time: 5.86310601234436\n",
      "losses: 7.000000359167062 1.0000052990420556\n",
      "grads: 9.99997276816361e-07 9.999938209698783e-07\n",
      "convexity test: 6.972762175258585 7.000000359167062\n",
      "gradient at x1 is small!\n",
      "gradient at x2 is small!\n",
      "The value between is not large enough. Find different points.\n",
      "\n",
      "\n",
      "=== TRIAL  1\n",
      "iteration information:  0 0 7.619222189923559 2.3389722764706935\n",
      "iteration information:  10000 11436 1.0000588196125715 1.1078210016677542e-05\n",
      "iteration information:  20000 21436 1.0000287722082233 5.423592286739665e-06\n",
      "iteration information:  30000 31436 1.0000190368751265 3.589785168691403e-06\n",
      "iteration information:  40000 41436 1.0000142220492654 2.682433444424412e-06\n",
      "iteration information:  50000 51436 1.0000113503058472 2.1411060760467216e-06\n",
      "iteration information:  60000 61436 1.0000094430672306 1.7815199544237748e-06\n",
      "iteration information:  70000 71436 1.0000080843618877 1.525315913811018e-06\n",
      "iteration information:  80000 81436 1.0000070673343011 1.3335184347223103e-06\n",
      "iteration information:  90000 91436 1.0000062775150107 1.1845555108644747e-06\n",
      "iteration information:  100000 101436 1.00000564642671 1.0655206341716899e-06\n",
      "done: 106520 107956 \t loss: 1.0000052990657378 \t   grad norm: 9.99998288228409e-07\n",
      "iteration information:  0 0 15.618330577262137 2.4065545645567488\n",
      "iteration information:  10000 10000 8.999999338343592 2.233695437515749e-06\n",
      "iteration information:  20000 20005 7.000000778776348 1.486629136643767e-06\n",
      "done: 23431 23436 \t loss: 7.000000523747021 \t   grad norm: 9.99925693950031e-07\n",
      "time: 5.997174263000488\n",
      "losses: 1.0000052990657378 7.000000523747021\n",
      "grads: 9.99998288228409e-07 9.99925693950031e-07\n",
      "convexity test: 6.999675014788087 7.000000523747021\n",
      "gradient at x1 is small!\n",
      "gradient at x2 is small!\n",
      "The value between is not large enough. Find different points.\n",
      "\n",
      "\n",
      "=== TRIAL  2\n",
      "iteration information:  0 0 14.918544304241927 2.6865449532538506\n",
      "iteration information:  10000 10001 7.0000012881574385 1.6149948228467228e-06\n",
      "done: 16076 16077 \t loss: 7.00000079749825 \t   grad norm: 9.999913662736492e-07\n",
      "iteration information:  0 0 12.956482260988693 2.2720595285695193\n",
      "done: 2563 2563 \t loss: 10.000000337493452 \t   grad norm: 9.999669336963458e-07\n",
      "time: 0.8890471458435059\n",
      "losses: 7.00000079749825 10.000000337493452\n",
      "grads: 9.999913662736492e-07 9.999669336963458e-07\n",
      "convexity test: 10.999999971987354 10.000000337493452\n",
      "gradient at x1 is small!\n",
      "gradient at x2 is small!\n",
      "The value between is significantly higher! Sucess!\n"
     ]
    }
   ],
   "source": [
    "# hmmm.... unless I'm lucky, trial and error is a lot of work... I'll make the computer do it instead...\n",
    "np.random.seed(123)\n",
    "dbg_founnd = -1\n",
    "for jtrial in range(1000):\n",
    "    print(\"\\n\\n=== TRIAL \",jtrial)\n",
    "    t0=time.time()\n",
    "    x1i = np.random.randn(d,1)\n",
    "    x2i = np.random.randn(d,1)\n",
    "    x1 = grad_descent(x1i, f, grad_f, tol=10**(-6),step_size=50,max_iters=200000)\n",
    "    x2 = grad_descent(x2i, f, grad_f, tol=10**(-6),step_size=50,max_iters=200000)\n",
    "    t1=time.time()\n",
    "    print(\"time:\",t1-t0)\n",
    "    print(\"losses:\", f(x1), f(x2))\n",
    "    print(\"grads:\", np.linalg.norm(grad_f(x1)), np.linalg.norm(grad_f(x2)))\n",
    "    print(\"convexity test:\", f((x1+x2)/2) , max(f(x1),f(x2)) )\n",
    "    \n",
    "    # I\n",
    "    if np.linalg.norm(grad_f(x1)) > 1e-6:\n",
    "        print (\"x1 failed gradient test\")\n",
    "        continue        \n",
    "    else:\n",
    "        print(\"gradient at x1 is small!\")\n",
    "        if np.linalg.norm(grad_f(x2)) > 1e-6:\n",
    "            print (\"x2 failed gradient test\")\n",
    "            continue\n",
    "        else:\n",
    "            print (\"gradient at x2 is small!\")\n",
    "            if f((x1+x2)/2) - max(f(x1),f(x2)) <= 0.1:\n",
    "                print (\"The value between is not large enough. Find different points.\")            \n",
    "                continue\n",
    "            else:\n",
    "                print (\"The value between is significantly higher! Sucess!\")\n",
    "                dbg_founnd = jtrial\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
